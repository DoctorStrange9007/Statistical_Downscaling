"""Core normalizing-flow and policy-gradient components for optimal transport.

This module implements:
- sinusoidal time embeddings for discrete time steps (as seperate function as it has multiple calls in separate modules)
- a conditional spline coupling flow (per-dimension rational-quadratic splines)
- a correlation network producing time- and state-dependent correlation `rho`
- a normalizing-flow model that samples sequences and computes log-probabilities
- a policy-gradient trainer that fits the flow to minimize transport cost and
  match the true data distribution (via negative log-likelihood), optionally
  with EMA parameters for evaluation and control variates for variance reduction

Shapes and conventions
----------------------
- Dimension `d`: data dimensionality for each time step.
- Time length `N_len = N + 1`: we index time `n` in [0, N].
- A trajectory in normalized coordinates has shape (N+1, d, 1).
- Sampling typically works in normalized space and is converted to ORIGINAL
  space via the transform method of the `DataNormalizer`.

External interfaces used
------------------------
- `DataNormalizer` from `preprocessing_OT` providing:
    - `.fit()` to compute dataset statistics
    - `.transform(mode, y, yp)` where mode in {"normalize","denormalize"}
    - attribute `log_det` used to adjust log-probabilities if normalization is on
- `true_data_model` providing:
    - `.sample_true_trajectory(key) -> (y, y')` in ORIGINAL space
"""

import jax
import jax.numpy as jnp
import jax.lax as lax
import haiku as hk
import distrax
import optax
from typing import Tuple
from functools import partial

from preprocessing_OT import DataNormalizer


def sinusoidal_time_embedding(n: jnp.ndarray, dim: int) -> jnp.ndarray:
    """Return sinusoidal time embedding for step index `n`.

    This mirrors positional encodings: for frequencies geometrically spaced
    between [1, 1/10000] we apply sin/cos and concatenate.

    Parameters
    ----------
    n : jnp.ndarray
        Scalar or array of time indices.
    dim : int
        Embedding dimension.

    Returns
    -------
    jnp.ndarray
        Embedding with shape `n.shape + (dim,)`.
    """
    dim = int(dim)
    half = dim // 2
    n = n.astype(jnp.float32)
    freqs = jnp.exp(
        -jnp.log(10000.0)
        * jnp.arange(0, half, dtype=jnp.float32)
        / jnp.maximum(half, 1)
    )
    args = n[..., None] * freqs[None, ...]
    emb = jnp.concatenate([jnp.sin(args), jnp.cos(args)], axis=-1)
    if dim % 2 == 1:
        emb = jnp.pad(emb, [(0, 0)] * (emb.ndim - 1) + [(0, 1)])
    return emb


class ConditionerMLP(hk.Module):
    """MLP producing spline parameters for a coupling flow conditioner.

    Given concatenated input `[x, context]`, outputs per-dimension parameters for
    a rational-quadratic spline: for each dimension we emit
    `3 * num_bins + 1` values (bin widths (K), bin heights (K), knot slopes ((K-1)(internal)+2(boundary))).
    """

    def __init__(self, name: str, d: int, num_bins: int, hidden_size: int):
        super().__init__(name=name)
        self.d = int(d)
        self.num_bins = int(num_bins)
        self.hidden_size = int(hidden_size)
        self.out_dim = self.d * (3 * self.num_bins + 1)

    def __call__(self, inp: jnp.ndarray) -> jnp.ndarray:
        """Return spline parameters shaped as (..., d, 3*num_bins + 1)."""
        h = hk.Linear(self.hidden_size)(inp)
        h = jax.nn.gelu(h)
        h = hk.Linear(self.hidden_size)(h)
        h = jax.nn.gelu(h)
        h = hk.Linear(
            self.out_dim,
            w_init=hk.initializers.Constant(0.0),
            b_init=hk.initializers.Constant(0.0),
        )(h)
        return jnp.reshape(h, h.shape[:-1] + (self.d, 3 * self.num_bins + 1))


class ConditionalSplineCouplingFlow(hk.Module):
    """Masked coupling flow with per-dimension rational-quadratic splines.

    The conditioner is an MLP that consumes `[x, context]` and outputs spline
    parameters. Layers alternate masks over dimensions.
    """

    def __init__(
        self,
        run_sett: dict,
        name: str,
        context_dim: int,
    ):
        super().__init__(name=name)
        self.run_sett_marginal_flow = run_sett["marginal_flow"]
        self.run_sett_global = run_sett["global"]
        self.d = int(self.run_sett_global["d"])
        self.context_dim = int(context_dim)
        self.num_layers = int(self.run_sett_marginal_flow["num_layers"])
        self.hidden_size = int(self.run_sett_marginal_flow["hidden_size"])
        self.num_bins = int(self.run_sett_marginal_flow["num_bins"])
        self.range_min = float(self.run_sett_marginal_flow["range_min"])
        self.range_max = float(self.run_sett_marginal_flow["range_max"])

        self._conds = [
            ConditionerMLP(f"cond_{i}", self.d, self.num_bins, self.hidden_size)
            for i in range(self.num_layers)
        ]
        self._masks = [
            (jnp.arange(self.d) % 2 == (i % 2)) for i in range(self.num_layers)
        ]

        self._base = distrax.MultivariateNormalDiag(
            loc=jnp.zeros((self.d,), dtype=jnp.float32),
            scale_diag=jnp.ones((self.d,), dtype=jnp.float32),
        )

    def _make_dist(self, context: jnp.ndarray) -> distrax.Transformed:
        """Construct the transformed distribution for a given `context`."""
        layers = []
        for i in range(self.num_layers):
            mask = self._masks[i]
            cond_mlp = self._conds[i]

            def _broadcast_context(ctx: jnp.ndarray, x: jnp.ndarray) -> jnp.ndarray:
                return jnp.broadcast_to(ctx, x.shape[:-1] + ctx.shape)

            def _conditioner(x, ctx=context, mlp=cond_mlp):
                ctx_b = _broadcast_context(ctx, x)
                inp = jnp.concatenate([x, ctx_b], axis=-1)
                return mlp(inp)

            layers.append(
                distrax.MaskedCoupling(
                    mask=mask,
                    conditioner=_conditioner,
                    bijector=lambda params: distrax.RationalQuadraticSpline(
                        params,
                        range_min=self.range_min,
                        range_max=self.range_max,
                        boundary_slopes="identity",
                        min_bin_size=1e-3,
                        min_knot_slope=1e-3,
                    ),
                )
            )
        return distrax.Transformed(self._base, distrax.Chain(layers))

    def forward_from_base(self, eps: jnp.ndarray, context: jnp.ndarray) -> jnp.ndarray:
        """Map base sample `eps` to data space given `context`."""
        dist = self._make_dist(context)
        return dist.bijector.forward(eps)

    def log_prob_and_base(
        self, x: jnp.ndarray, context: jnp.ndarray
    ) -> Tuple[jnp.ndarray, jnp.ndarray]:
        """Return log-probability of `x` and corresponding base variable `u`."""
        dist = self._make_dist(context)
        u, ildj = dist.bijector.inverse_and_log_det(x)
        logp = self._base.log_prob(u) + ildj
        return logp, u


class RhoNet(hk.Module):
    """Network producing correlation vector `rho` in [-rho_max, rho_max]^d.

    Inputs are previous y, previous y' and a time embedding for step `n`. Output
    is clipped to [-rho_max, rho_max].
    """

    def __init__(self, run_sett: dict, name: str):
        super().__init__(name=name)
        self.run_sett_global = run_sett["global"]
        self.run_sett_correlation_flow = run_sett["correlation_flow"]
        self.d = int(self.run_sett_global["d"])
        self.time_emb_dim = int(self.run_sett_global["time_emb_dim"])
        self.hidden_size = int(self.run_sett_correlation_flow["hidden_size"])
        self.rho_max = float(self.run_sett_correlation_flow["rho_max"])

    def __call__(
        self, prev_y: jnp.ndarray, prev_yp: jnp.ndarray, n: jnp.ndarray
    ) -> jnp.ndarray:
        """Compute per-dimension correlation `rho` for time step `n`."""
        te = sinusoidal_time_embedding(n, self.time_emb_dim).reshape((-1,))
        inp = jnp.concatenate([prev_y, prev_yp, te], axis=0)
        h = hk.Linear(self.hidden_size)(inp)
        h = jax.nn.gelu(h)
        h = hk.Linear(self.hidden_size)(h)
        h = jax.nn.gelu(h)
        out = hk.Linear(self.d)(h)
        rho = self.rho_max * jnp.tanh(out)
        return jnp.clip(rho, -self.rho_max, self.rho_max)


class NormalizingFlowModel:
    """Normalizing-flow model to sample sequences and compute log-probabilities.

    The model maintains two conditional flows (for y and y') and a correlation
    network that couples their base variables via a Gaussian copula with
    parameter `rho`. Sampling/log-likelihood computations are performed in the
    normalized space and converted if normalization is enabled.
    """

    def __init__(self, run_sett: dict, true_data_model):
        self.run_sett = run_sett
        self.run_sett_global = run_sett["global"]
        self.run_sett_marginal_flow = run_sett["marginal_flow"]
        self.run_sett_correlation_flow = run_sett["correlation_flow"]
        self.run_sett_preprocessing = run_sett["preprocessing"]

        self.d = int(self.run_sett_global["d"])
        self.N = int(self.run_sett_global["N"])
        self.N_len = int(self.N + 1)
        self.seed = int(self.run_sett_global["seed"])
        self.time_emb_dim = int(self.run_sett_global["time_emb_dim"])

        self.num_layers = int(self.run_sett_marginal_flow["num_layers"])
        self.hidden_size = int(self.run_sett_marginal_flow["hidden_size"])
        self.num_bins = int(self.run_sett_marginal_flow["num_bins"])

        self.rho_hidden = int(self.run_sett_correlation_flow["rho_hidden"])
        self.rho_max = float(self.run_sett_correlation_flow["rho_max"])

        self.normalizer = DataNormalizer(run_sett, true_data_model).fit()
        self.use_data_normalization = bool(
            self.run_sett_preprocessing["use_data_normalization"]
        )

        self._hk_sample_norm = hk.without_apply_rng(
            hk.transform(self._sample_batch_norm_impl)
        )
        self._hk_logprob_steps_norm = hk.without_apply_rng(
            hk.transform(self._logprob_steps_batch_norm_impl)
        )
        self._hk_logprob_total_norm = hk.without_apply_rng(
            hk.transform(self._logprob_total_batch_norm_impl)
        )
        init_key = jax.random.PRNGKey(self.seed)
        dummy_keys = jax.random.split(init_key, 2)  # 2 needed?
        self.params = self._hk_sample_norm.init(init_key, keys=dummy_keys)

    def _ctx(self, prev: jnp.ndarray, n: jnp.ndarray) -> jnp.ndarray:
        """Concatenate previous state with sinusoidal time embedding."""
        te = sinusoidal_time_embedding(n, self.time_emb_dim).reshape((-1,))
        return jnp.concatenate([prev, te], axis=0)

    def _make_modules(self):
        """Create conditional flows for y and y' and the correlation network."""
        ctx_dim = self.d + self.time_emb_dim
        y_flow = ConditionalSplineCouplingFlow(
            run_sett=self.run_sett,
            name="y_flow",
            context_dim=ctx_dim,
        )
        yp_flow = ConditionalSplineCouplingFlow(
            run_sett=self.run_sett,
            name="yp_flow",
            context_dim=ctx_dim,
        )
        rho_net = RhoNet(
            run_sett=self.run_sett,
            name="rho_net",
        )
        return y_flow, yp_flow, rho_net

    def _sample_one_traj_norm(self, y_flow, yp_flow, rho_net, key: jax.Array):
        """Sample a single normalized trajectory (y, y') and return mean |rho|."""
        d, N_len = self.d, self.N_len

        def step(carry, n):
            prev_y, prev_yp, key_in, rho_acc = carry
            key_in, k1, k2 = jax.random.split(key_in, 3)

            rho = rho_net(prev_y, prev_yp, n)
            eps1 = jax.random.normal(k1, (d,))
            eps2 = jax.random.normal(k2, (d,))

            s = jnp.sqrt(jnp.clip(1.0 - rho * rho, 1e-6, 1.0))
            u = eps1
            v = rho * eps1 + s * eps2

            ctx_y = self._ctx(prev_y, n)
            ctx_yp = self._ctx(prev_yp, n)
            y_n = y_flow.forward_from_base(u, ctx_y)
            yp_n = yp_flow.forward_from_base(v, ctx_yp)

            rho_acc = rho_acc + jnp.mean(jnp.abs(rho))
            return (y_n, yp_n, key_in, rho_acc), (y_n, yp_n)

        prev0 = jnp.zeros((d,), dtype=jnp.float32)
        carry0 = (prev0, prev0, key, jnp.array(0.0, dtype=jnp.float32))
        ns = jnp.arange(N_len, dtype=jnp.int32)
        (_, _, _, rho_sum), (ys, yps) = hk.scan(step, carry0, ns)

        mean_abs_rho = rho_sum / N_len
        y_traj = ys.reshape((N_len, d, 1))
        yp_traj = yps.reshape((N_len, d, 1))
        return y_traj, yp_traj, mean_abs_rho

    def _logprob_steps_one_traj_norm(self, y_flow, yp_flow, rho_net, y_traj, yp_traj):
        """Compute per-step log-probabilities for a normalized `(y,y')` trajectory."""
        d, N_len = self.d, self.N_len
        y_vec = jnp.squeeze(y_traj, axis=-1)
        yp_vec = jnp.squeeze(yp_traj, axis=-1)

        def step(carry, n):
            prev_y, prev_yp = carry
            y_n = y_vec[n]
            yp_n = yp_vec[n]

            rho = rho_net(prev_y, prev_yp, n)
            ctx_y = self._ctx(prev_y, n)
            ctx_yp = self._ctx(prev_yp, n)

            logpy, u = y_flow.log_prob_and_base(y_n, ctx_y)
            logpyp, v = yp_flow.log_prob_and_base(yp_n, ctx_yp)

            def _gaussian_copula_correction(
                u: jnp.ndarray, v: jnp.ndarray, rho: jnp.ndarray
            ) -> jnp.ndarray:
                rho = jnp.clip(rho, -0.999, 0.999)
                one_minus = jnp.clip(1.0 - rho * rho, 1e-6, 1.0)
                quad = (u * u - 2.0 * rho * u * v + v * v) / one_minus
                corr = -0.5 * jnp.log(one_minus) - 0.5 * quad + 0.5 * (u * u + v * v)
                return jnp.sum(corr)

            corr = _gaussian_copula_correction(u, v, rho)

            logp_n = logpy + logpyp + corr
            return (y_n, yp_n), logp_n

        prev0 = jnp.zeros((d,), dtype=jnp.float32)
        ns = jnp.arange(N_len, dtype=jnp.int32)
        (_, _), logp_steps = hk.scan(step, (prev0, prev0), ns)
        return logp_steps

    def _sample_batch_norm_impl(self, keys: jax.Array):
        """Haiku-transformed: vmap of `_sample_one_traj_norm` over `keys`."""
        y_flow, yp_flow, rho_net = self._make_modules()

        def one(k):
            y_z, yp_z, mean_abs_rho = self._sample_one_traj_norm(
                y_flow, yp_flow, rho_net, k
            )
            return y_z, yp_z, mean_abs_rho

        return hk.vmap(one, split_rng=False)(keys)

    def _logprob_steps_batch_norm_impl(self, y_z: jnp.ndarray, yp_z: jnp.ndarray):
        """Haiku-transformed: vmap per-step log-probs over a batch of trajectories."""
        y_flow, yp_flow, rho_net = self._make_modules()

        def one(a, b):
            return self._logprob_steps_one_traj_norm(y_flow, yp_flow, rho_net, a, b)

        return hk.vmap(one, split_rng=False)(y_z, yp_z)

    def _logprob_total_batch_norm_impl(self, y_z: jnp.ndarray, yp_z: jnp.ndarray):
        """Haiku-transformed: sum step log-probs over time for each trajectory."""
        steps = self._logprob_steps_batch_norm_impl(y_z, yp_z)
        return jnp.sum(steps, axis=1)

    def sample_batch_norm(self, params: hk.Params, keys: jax.Array):
        """Sample a batch of normalized trajectories `(y_z, yp_z, mean_abs_rho)`."""
        return self._hk_sample_norm.apply(params, keys=keys)

    def logprob_steps_batch_norm(
        self, params: hk.Params, y_z: jnp.ndarray, yp_z: jnp.ndarray
    ):
        """Compute per-step log-probabilities for a batch of normalized trajectories."""
        return self._hk_logprob_steps_norm.apply(params, y_z=y_z, yp_z=yp_z)

    def logprob_total_batch_norm(
        self, params: hk.Params, y_z: jnp.ndarray, yp_z: jnp.ndarray
    ):
        """Compute total log-probabilities for a batch of normalized trajectories."""
        return self._hk_logprob_total_norm.apply(params, y_z=y_z, yp_z=yp_z)


class PolicyGradient:
    """Policy-gradient trainer for the normalizing-flow sequence model.

    Optimizes a mixture of:
    - score-function surrogate cost with advantage weighting and optional control variates
    - pathwise transport cost
    - negative log-likelihood on true data with weight beta

    Maintains an EMA of parameters for evaluation if configured.
    """

    def __init__(self, run_sett: dict, true_data_model, normalizing_flow_model):
        self.run_sett = run_sett
        self.run_sett_global = run_sett["global"]
        self.run_sett_beta = run_sett["beta_schedule"]
        self.run_sett_lr = run_sett["lr_schedule"]
        self.run_sett_policy_gradient = run_sett["policy_gradient"]
        self.run_sett_metrics = run_sett["metrics"]
        self.run_sett_baseline_fitting = run_sett["baseline_fitting"]
        self.run_sett_ema = run_sett["ema"]

        self.d = int(self.run_sett_global["d"])
        self.N = int(self.run_sett_global["N"])
        self.N_len = int(self.N + 1)
        self.B = int(self.run_sett_global["B"])
        self.seed = int(self.run_sett_global["seed"])
        self.time_emb_dim = int(self.run_sett_global["time_emb_dim"])
        self.cv_ridge = float(self.run_sett_baseline_fitting["cv_ridge"])
        self.cv_split_ratio = float(self.run_sett_baseline_fitting["cv_split_ratio"])
        self._B_fit_static = int(
            max(0, min(self.B, round(self.cv_split_ratio * self.B)))
        )

        self.true_data_model = true_data_model
        self.model = normalizing_flow_model

        self.ema_decay = float(self.run_sett_ema["ema_decay"])
        self.use_ema_eval = bool(self.run_sett_ema["use_ema_eval"])
        self.params = self.model.params
        self.ema_params = jax.tree_util.tree_map(lambda x: x, self.params)

        self.beta_schedule = self._build_beta_schedule()
        self._last_beta_value = float(self.beta_schedule(0))
        self.kl_warmup_steps = int(self.run_sett_beta["kl_only_warmup_steps"])

        self.lrate_schedule = self._build_lr_schedule()
        self.optimizer = optax.adam(learning_rate=self.lrate_schedule)
        self.opt_state = self.optimizer.init(self.params)

        self.mix_pathwise_alpha = float(
            self.run_sett_policy_gradient["mix_pathwise_alpha"]
        )
        self.use_control_variates = bool(
            self.run_sett_policy_gradient["use_control_variates"]
        )
        self.use_advantage_standardization = bool(
            self.run_sett_policy_gradient["use_advantage_standardization"]
        )

        self._step = 0

    def _build_beta_schedule(self):
        """Create a beta schedule for the NLL weight."""
        mode_type = str(self.run_sett_beta["type"]).lower()
        init_beta = float(self.run_sett_beta["init_boundary_value"])
        end_beta = float(self.run_sett_beta["end_boundary_value"])
        if mode_type == "constant":
            return optax.constant_schedule(end_beta)
        if mode_type == "linear":
            num_iter = int(self.run_sett_global["num_iterations"])
            warmup_ratio = float(self.run_sett_beta["warmup_end_ratio"])
            ramp_steps = max(1, int(num_iter * warmup_ratio))
            return optax.linear_schedule(init_beta, end_beta, ramp_steps)
        if mode_type == "schedule":
            num_iter = int(self.run_sett_beta)
            start_ramp = int(num_iter * 0.2)
            end_ramp = int(num_iter * 0.8)
            ramp = max(1, end_ramp - start_ramp)
            return optax.join_schedules(
                schedules=[
                    optax.constant_schedule(init_beta),
                    optax.linear_schedule(init_beta, end_beta, ramp),
                    optax.constant_schedule(end_beta),
                ],
                boundaries=[start_ramp, end_ramp],
            )
        return optax.constant_schedule(end_beta)

    def _build_lr_schedule(self):
        """Create learning-rate schedule (constant or cosine with warmup/decay)."""
        init_value = float(self.run_sett_lr["init_value"])
        peak_value = float(self.run_sett_lr["peak_value"])
        warmup_steps = int(self.run_sett_lr["warmup_steps"])
        decay_steps = int(self.run_sett_lr["decay_steps"])
        end_value = float(self.run_sett_lr["end_value"])
        constant_lr = float(self.run_sett_lr["constant_lr"])
        mode_type = str(self.run_sett_lr["type"]).lower()
        if mode_type == "constant":
            return optax.constant_schedule(constant_lr)
        elif mode_type == "cosine":
            warmup = optax.linear_schedule(
                init_value=init_value,
                end_value=peak_value,
                transition_steps=max(warmup_steps, 1),
            )
            tail = optax.cosine_decay_schedule(
                init_value=peak_value,
                decay_steps=max(decay_steps, 1),
                alpha=end_value / max(peak_value, 1e-12),
            )

            return optax.join_schedules(
                schedules=[warmup, tail], boundaries=[max(warmup_steps, 1)]
            )

    def get_eval_params_trees(self):
        """Return EMA params if enabled, else current params."""
        return self.ema_params if self.use_ema_eval else self.params

    def _phi(
        self, prev_y_z: jnp.ndarray, prev_yp_z: jnp.ndarray, t: jnp.ndarray
    ) -> jnp.ndarray:
        """Feature vector for baseline fitting at step `t` given previous states."""
        prev_y = jnp.squeeze(prev_y_z, -1)
        prev_yp = jnp.squeeze(prev_yp_z, -1)
        te = sinusoidal_time_embedding(t, self.time_emb_dim).reshape((-1,))
        return jnp.concatenate(
            [jnp.ones((1,), dtype=jnp.float32), prev_y, prev_yp, te], axis=0
        )

    def _fit_baseline_per_n(self, phi: jnp.ndarray, target: jnp.ndarray, ridge: float):
        """
        phi: (B,N+1,p), target: (B,N+1) -> w: (N+1,p)
        """
        B, N_len, p = phi.shape

        def solve_one(n):
            X = phi[:, n, :]
            y = target[:, n]
            XtX = X.T @ X + ridge * jnp.eye(p, dtype=X.dtype)
            Xty = X.T @ y
            return jnp.linalg.solve(XtX, Xty)

        ns = jnp.arange(N_len, dtype=jnp.int32)
        return jax.vmap(solve_one)(ns)

    def _baseline_predict(self, phi: jnp.ndarray, w: jnp.ndarray):
        """Predict baseline values given features `phi` and weights `w`."""
        return jnp.einsum("bnp,np->bn", phi, w)

    @partial(jax.jit, static_argnums=0)
    def _train_step(
        self, params, ema_params, opt_state, key: jax.Array, step_i: jnp.ndarray
    ):
        """One training step: compute loss, update params and EMA, return metrics."""
        beta = jnp.asarray(self.beta_schedule(step_i), dtype=jnp.float32)
        warm = jnp.asarray(self.kl_warmup_steps, dtype=jnp.int32)
        beta = jnp.where(step_i < warm, jnp.array(0.0, dtype=jnp.float32), beta)

        key, k_model, k_true = jax.random.split(key, 3)
        keys_m = jax.random.split(k_model, self.B)
        keys_t = jax.random.split(k_true, self.B)

        y_true, yp_true = jax.vmap(self.true_data_model.sample_true_trajectory)(keys_t)
        y_true_z, yp_true_z = self.model.normalizer.transform(
            "normalize", y_true, yp_true
        )

        alpha = jnp.clip(
            jnp.asarray(self.mix_pathwise_alpha, dtype=jnp.float32), 0.0, 1.0
        )

        def loss_and_aux(p):
            y_z_s, yp_z_s, rho_abs = self.model.sample_batch_norm(p, keys_m)

            y_z_ng = lax.stop_gradient(y_z_s)
            yp_z_ng = lax.stop_gradient(yp_z_s)

            y_ng, yp_ng = self.model.normalizer.transform(
                "denormalize", y_z_ng, yp_z_ng
            )
            sq = 0.5 * jnp.sum((y_ng - yp_ng) ** 2, axis=(2, 3))
            V = jnp.flip(jnp.cumsum(jnp.flip(sq, axis=1), axis=1), axis=1)

            if self.use_control_variates:
                prev0 = jnp.zeros((self.d, 1), dtype=jnp.float32)

                def build_phi_one_traj(yz, ypz):
                    def step(carry, n):
                        prev_y, prev_yp = carry
                        cur_phi = self._phi(prev_y, prev_yp, n)
                        return (yz[n], ypz[n]), cur_phi

                    ns = jnp.arange(self.N_len, dtype=jnp.int32)
                    (_, _), phis = lax.scan(step, (prev0, prev0), ns)
                    return phis

                phi = jax.vmap(build_phi_one_traj)(y_z_ng, yp_z_ng)

                if self._B_fit_static <= 0 or self._B_fit_static >= self.B:
                    w = self._fit_baseline_per_n(phi, V, ridge=self.cv_ridge)
                    baseline = self._baseline_predict(phi, w)
                else:
                    phi_fit = phi[: self._B_fit_static]
                    V_fit = V[: self._B_fit_static]
                    w = self._fit_baseline_per_n(phi_fit, V_fit, ridge=self.cv_ridge)
                    baseline = self._baseline_predict(phi, w)
            else:
                baseline = jnp.zeros_like(V)

            Adv = V - baseline

            if self.use_advantage_standardization:
                mean_t = jnp.mean(Adv, axis=0, keepdims=True)
                std_t = jnp.std(Adv, axis=0, keepdims=True) + 1e-6
                Adv = (Adv - mean_t) / std_t

            Adv = lax.stop_gradient(Adv)

            # ---- score-function surrogate ----
            logp_steps = self.model.logprob_steps_batch_norm(p, y_z_ng, yp_z_ng)
            loss_cost_sf = jnp.mean(jnp.sum(Adv * logp_steps, axis=1))

            # ---- pathwise true cost ----
            y_s, yp_s = self.model.normalizer.transform("denormalize", y_z_s, yp_z_s)
            V0_pw = 0.5 * jnp.sum((y_s - yp_s) ** 2, axis=(1, 2, 3))
            cost_mean_pathwise = jnp.mean(V0_pw)

            loss_cost = (1.0 - alpha) * loss_cost_sf + alpha * cost_mean_pathwise

            logp_total_z = self.model.logprob_total_batch_norm(p, y_true_z, yp_true_z)
            if self.model.use_data_normalization:
                assert self.model.normalizer is not None
                logp_total = logp_total_z - self.model.normalizer.log_det
            else:
                logp_total = logp_total_z
            nll_true = -jnp.mean(logp_total)

            loss = loss_cost + beta * nll_true

            aux = {
                "loss": loss,
                "loss_cost_sf": loss_cost_sf,
                "cost_mean_pathwise": cost_mean_pathwise,
                "alpha_mix": alpha,
                "nll_true": nll_true,
                "beta": beta,
                "mean_abs_rho_model": jnp.mean(rho_abs),
            }
            return loss, aux

        (loss_val, aux), grads = jax.value_and_grad(loss_and_aux, has_aux=True)(params)

        updates, opt_state = self.optimizer.update(grads, opt_state, params)
        params = optax.apply_updates(params, updates)

        ema_params = jax.tree_util.tree_map(
            lambda e, p: self.ema_decay * e + (1.0 - self.ema_decay) * p,
            ema_params,
            params,
        )

        return params, ema_params, opt_state, aux, beta

    def update_params(self, key: jax.Array):
        """Advance training by one step and return scalar metrics."""
        step_i = jnp.asarray(self._step, dtype=jnp.int32)
        self.params, self.ema_params, self.opt_state, metrics, beta = self._train_step(
            self.params, self.ema_params, self.opt_state, key, step_i
        )
        self._last_beta_value = float(beta)
        self._step += 1
        return {k: float(v) for k, v in metrics.items()}

    def compute_logging_losses(self, key: jax.Array):
        """Compute logging metrics J_val, NLL_true, and combined J_beta."""
        rng_namespace = int(self.run_sett_policy_gradient["RNG_NAMESPACE_PG"])
        B = int(self.run_sett_metrics["num_samples"])
        chunk = int(self.run_sett_metrics["chunk_size"])
        params = self.get_eval_params_trees()

        remaining = B
        cur_key = jax.random.fold_in(key, rng_namespace)
        cost_sum = 0.0
        tot = 0
        while remaining > 0:
            cur = min(remaining, chunk)
            cur_key, use_key = jax.random.split(cur_key)
            keys = jax.random.split(use_key, cur)
            y_z, yp_z, _ = self.model.sample_batch_norm(params, keys)
            y, yp = self.model.normalizer.transform("denormalize", y_z, yp_z)
            diff = y - yp
            V0 = 0.5 * jnp.sum(diff * diff, axis=(1, 2, 3))
            cost_sum += float(jnp.sum(V0))
            tot += cur
            remaining -= cur
        J_val = cost_sum / max(tot, 1)

        remaining = B
        cur_key = jax.random.fold_in(key, rng_namespace + 111_111)
        nll_sum = 0.0
        tot = 0
        while remaining > 0:
            cur = min(remaining, chunk)
            cur_key, use_key = jax.random.split(cur_key)
            keys = jax.random.split(use_key, cur)
            y_t, yp_t = jax.vmap(self.true_data_model.sample_true_trajectory)(keys)
            y_z, yp_z = self.model.normalizer.transform("normalize", y_t, yp_t)
            logp_z = self.model.logprob_total_batch_norm(params, y_z, yp_z)
            if self.model.use_data_normalization:
                assert self.model.normalizer is not None
                logp = logp_z - self.model.normalizer.log_det
            else:
                logp = logp_z
            nll_sum += float(jnp.sum(-logp))
            tot += cur
            remaining -= cur
        NLL_true = nll_sum / max(tot, 1)

        beta_now = float(self._last_beta_value)
        J_beta = J_val + beta_now * NLL_true
        return {
            "J_val": float(J_val),
            "NLL_true": float(NLL_true),
            "J_beta": float(J_beta),
            "beta_now": float(beta_now),
        }

    def sample_trajectories(
        self,
        key: jax.Array,
        num: int,
        params: hk.Params,
    ):
        """
        Returns trajectories in ORIGINAL space:
          y:  (num, N+1, d, 1)
          yp: (num, N+1, d, 1)
        """
        num = int(num)
        keys = jax.random.split(key, num)
        y_z, yp_z, _ = self.model.sample_batch_norm(params, keys)
        y, yp = self.model.normalizer.transform("denormalize", y_z, yp_z)
        return y, yp
